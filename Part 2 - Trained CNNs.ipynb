{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce52e4cd",
   "metadata": {},
   "source": [
    "`Rasmus Utz Faber`\n",
    "\n",
    "`Martín Omil Nogales`\n",
    "\n",
    "# Unit 3 - Part 2: Trained CNNs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef095f6a",
   "metadata": {},
   "source": [
    "## 1. Selection of models, data extraction and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f05c123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 00:02:33.905847: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-16 00:02:33.921500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742079753.938576 3521788 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742079753.943759 3521788 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-16 00:02:33.961179: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load and Preprocess Data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Extract a validation set from the training set (10%)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2281d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742079759.833325 3521788 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13536 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:98:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Resize images to 224x224 to match most pretrained models\n",
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "def resize_images(images):\n",
    "    images_resized = np.array([tf.image.resize(image, IMG_SIZE).numpy() for image in images])\n",
    "    return images_resized\n",
    "\n",
    "x_train = resize_images(x_train)\n",
    "x_val = resize_images(x_val)\n",
    "x_test = resize_images(x_test)\n",
    "\n",
    "# Data Augmentation Layer\n",
    "# data_augmentation = keras.Sequential([\n",
    "#     keras.layers.RandomFlip(\"horizontal\"),\n",
    "#     keras.layers.RandomRotation(0.2),\n",
    "#     keras.layers.RandomZoom(0.2),\n",
    "#     keras.layers.RandomTranslation(0.1, 0.1)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64dcf6d",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a15955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Extraction (Initial Setup)\n",
    "def build_feature_extractor(model_name):\n",
    "    base_model = None\n",
    "\n",
    "    if model_name == 'ResNet50V2':\n",
    "        base_model = keras.applications.ResNet50V2(weights=\"imagenet\", include_top=False, input_shape=(128, 128, 3))\n",
    "    elif model_name == 'VGG19':\n",
    "        base_model = keras.applications.VGG19(weights=\"imagenet\", include_top=False, input_shape=(128, 128, 3))\n",
    "    elif model_name == 'MobileNet':\n",
    "        base_model = keras.applications.MobileNet(weights=\"imagenet\", include_top=False, input_shape=(128, 128, 3))\n",
    "    elif model_name == 'InceptionV3':\n",
    "        base_model = keras.applications.InceptionV3(weights=\"imagenet\", include_top=False, input_shape=(128, 128, 3))\n",
    "    elif model_name == 'ResNet101V2':\n",
    "        base_model = keras.applications.ResNet101V2(weights=\"imagenet\", include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "    base_model.trainable = False  # Freeze the pretrained model\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        #data_augmentation,  # Applying Data Augmentation\n",
    "        base_model,\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.Dense(512, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(100, activation='softmax')  # CIFAR-100 has 100 classes\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e60c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Training the models for Feature Extraction\n",
    "\n",
    "def train_model(model_name):\n",
    "    model = build_feature_extractor(model_name)\n",
    "    print(f\"\\nTraining {model_name} with Feature Extraction...\")\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=3,\n",
    "        batch_size=128\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(f'{model_name}_feature_extractor.keras')\n",
    "    print(f\"Model saved as {model_name}_feature_extractor.keras\")\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print(f\"{model_name} Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "    return history, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611562f3",
   "metadata": {},
   "source": [
    "## 3. Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7c2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Fine-Tuning the models\n",
    "\n",
    "def fine_tune_model(model_name):\n",
    "    model = keras.models.load_model(f'{model_name}_feature_extractor.keras')\n",
    "\n",
    "    # Unfreeze some layers (last 2 blocks)\n",
    "    base_model = model.layers[0]\n",
    "    base_model.trainable = True\n",
    "\n",
    "    for layer in base_model.layers[:-30]:  # Keep most layers frozen\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compile the model with a low learning rate\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(f\"\\nFine-tuning {model_name}...\")\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=3,\n",
    "        batch_size=128\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print(f\"{model_name} Fine-Tuning Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "    return history, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e4b6b",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da935e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ResNet50V2 with Feature Extraction...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742079809.991877 3521829 service.cc:148] XLA service 0x152754013360 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742079809.991903 3521829 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-03-16 00:03:30.120814: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1742079811.017513 3521829 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-03-16 00:03:35.203852: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.80GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  2/352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 82ms/step - accuracy: 0.0117 - loss: 6.0978   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742079817.120826 3521829 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2914 - loss: 3.1241"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 00:04:09.484327: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.68GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 133ms/step - accuracy: 0.2920 - loss: 3.1207 - val_accuracy: 0.5228 - val_loss: 1.7635\n",
      "Epoch 2/3\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 96ms/step - accuracy: 0.5164 - loss: 1.8043 - val_accuracy: 0.5478 - val_loss: 1.6177\n",
      "Epoch 3/3\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 97ms/step - accuracy: 0.5678 - loss: 1.5488 - val_accuracy: 0.5560 - val_loss: 1.6176\n",
      "Model saved as ResNet50V2_feature_extractor.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - accuracy: 0.5554 - loss: 1.5931\n",
      "ResNet50V2 Test Accuracy: 55.75%\n",
      "\n",
      "Fine-tuning ResNet50V2...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 00:06:52.518831: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.53GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.5113 - loss: 1.8663"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 00:07:42.904339: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.52GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 178ms/step - accuracy: 0.5115 - loss: 1.8653 - val_accuracy: 0.5678 - val_loss: 1.5810\n",
      "Epoch 2/3\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 140ms/step - accuracy: 0.6256 - loss: 1.3252 - val_accuracy: 0.5876 - val_loss: 1.5000\n",
      "Epoch 3/3\n",
      "\u001b[1m113/352\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 124ms/step - accuracy: 0.6726 - loss: 1.1367"
     ]
    }
   ],
   "source": [
    "# Models to train and fine-tune\n",
    "model_names = ['ResNet50V2', 'InceptionV3'] #, 'MobileNet']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Feature Extraction\n",
    "    history, test_acc = train_model(model_name)\n",
    "    results[f'{model_name} (Feature Extraction)'] = test_acc\n",
    "\n",
    "    # Fine-Tuning\n",
    "    history, test_acc = fine_tune_model(model_name)\n",
    "    results[f'{model_name} (Fine-Tuning)'] = test_acc\n",
    "\n",
    "print(\"\\nAll Results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7520a65c",
   "metadata": {},
   "source": [
    "## 5. Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5104c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate Performance Comparison Graph\n",
    "labels = list(results.keys())\n",
    "scores = list(results.values())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(labels, scores, color='skyblue')\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.title('Performance Comparison of Pretrained Models on CIFAR-100')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c0f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Generate Performance Comparison Table\n",
    "results_df = pd.DataFrame(list(results.items()), columns=['Model and Strategy', 'Test Accuracy'])\n",
    "\n",
    "print(\"\\nPerformance Comparison Table:\\n\")\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
